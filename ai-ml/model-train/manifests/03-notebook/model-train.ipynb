{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8bf6b1",
   "metadata": {},
   "source": [
    " Copyright 2024 Google LLC\n",
    "\n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " you may not use this file except in compliance with the License.\n",
    " You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad9294",
   "metadata": {},
   "source": [
    "This sections shows how to run model training using PyTorch and data from specific storage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fce6c7",
   "metadata": {},
   "source": [
    "Import required python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3c49c7-f0e3-4d8b-b631-bf3f0a92828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71898099",
   "metadata": {},
   "source": [
    "Declare batch size for dataset reading. Define dataset [transformation parameters](https://pytorch.org/vision/stable/transforms.html) - resize image, apply random image augmentations, convert to tensor and normalize it to fit all vector dimensions into [-1, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67fd906d-1d87-44d0-a64a-484dfab4d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "    transforms.RandomRotation(degrees=(30, 70)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac6430",
   "metadata": {},
   "source": [
    "Declare custom neural network. Worth to mention nn.Linear parameter set limits to input features (vector dimensions of input image) and output (corresponding image class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d15daf-eeab-4840-a3a5-f68f365754fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 1000)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd39481",
   "metadata": {},
   "source": [
    "In this cell you check if CUDA is available and declare two [optimization functions](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) - optimizer (the Adam\n",
    "optimizer with a 0.001 learning rate) and criterion (the Cross-Entropy loss function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0a5b27-b699-4c14-9fe2-a36d992079b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Computation device: {device}\\n\")\n",
    "model = CNNModel().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c187a",
   "metadata": {},
   "source": [
    "Define helper save_model function to save model states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "178a81c6-dbc4-427a-8f82-7cf503d423b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epochs, model, optimizer, criterion):\n",
    "    model_path = \"/local-ssd/outputs/model-\" + time.strftime(\"%H-%M-%S\", time.localtime()) + \".pth\"\n",
    "    torch.save({\n",
    "                'epoch': epochs,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, model_path)\n",
    "    print(f\"Model was saved in {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b401f2ad",
   "metadata": {},
   "source": [
    "Define model training function that runs very [common training loop](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0b54e29-fe75-4a98-b636-c85ee7945e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    print('Training')\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "        counter += 1\n",
    "        image, labels = data\n",
    "        image = image.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_correct += (preds == labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss = train_running_loss / counter\n",
    "    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42c785",
   "metadata": {},
   "source": [
    "Declare benchmark function, that calculates training time using different storages and epochs amount. The function erases cache in GPU memory and uploads an untrained model to GPU memory before each training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd69f8-4018-4fad-ae91-faa30b13f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_benchmark(dataset_path, epochs):\n",
    "    start = time.time()\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=dataset_path,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    #recreate the model before training\n",
    "    torch.cuda.empty_cache()\n",
    "    model = CNNModel().to(device)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "        train_epoch_loss, train_epoch_acc = train(model, train_loader, optimizer, criterion)\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        train_acc.append(train_epoch_acc)\n",
    "        print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "        print('-'*50)\n",
    "        save_model(epochs, model, optimizer, criterion)\n",
    "    print('Training complete')\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Total training time: \", time.strftime(\"%H:%M:%S\", time.gmtime(end-start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23f203-908f-4e41-82e4-803683ce20f8",
   "metadata": {},
   "source": [
    "Run the benchmark using Ram disk and 2/5/10 training cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4762d0ec-8c0c-40c2-8930-8feb8e81e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ram disk - 2 epochs\")\n",
    "train_benchmark(\"/ram-disk/dataset\", 2)\n",
    "print(\"Ram disk - 5 epochs\")\n",
    "train_benchmark(\"/ram-disk/dataset\", 5)\n",
    "print(\"Ram disk - 10 epochs\")\n",
    "train_benchmark(\"/ram-disk/dataset\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b7e9a-14d4-4d3c-b20a-9987c46e78b0",
   "metadata": {},
   "source": [
    "Run the benchmark using Local SSD and 2/5/10 training cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908c503-4b86-451e-bd04-ff00e65a2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Local ssd - 2 epochs\")\n",
    "train_benchmark(\"/local-ssd/dataset\", 2)\n",
    "print(\"Local ssd - 5 epochs\")\n",
    "train_benchmark(\"/local-ssd/dataset\", 5)\n",
    "print(\"Local ssd - 10 epochs\")\n",
    "train_benchmark(\"/local-ssd/dataset\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1d7bc-942e-477e-8c81-43eb70ccc981",
   "metadata": {},
   "source": [
    "Run the benchmark using Persistent disk and 2/5/10 training cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322de896-0b78-48e1-829e-363b1577dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Persistent disk - 2 epochs\")\n",
    "train_benchmark(\"/pd-ssd/dataset\", 2)\n",
    "print(\"Persistent disk - 5 epochs\")\n",
    "train_benchmark(\"/pd-ssd/dataset\", 5)\n",
    "print(\"Persistent disk - 2 epochs\")\n",
    "train_benchmark(\"/pd-ssd/dataset\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402ef4a7-3c5e-4dbf-bed7-ffb3a9d04f65",
   "metadata": {},
   "source": [
    "Run the benchmark using GCS bucket and 2/5/10 training cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d258f2-41b4-4e81-9120-e0e1e8ba1d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Local ssd - 2 epochs\")\n",
    "train_benchmark(\"/bucket/datase\", 2)\n",
    "print(\"Ram disk - 5 epochs\")\n",
    "train_benchmark(\"/bucket/datase\", 5)\n",
    "print(\"Bucket - 10 epochs\")\n",
    "train_benchmark(\"/bucket/dataset\", 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
