{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install **kubectl** and the **Google Cloud SDK** with the necessary authentication plugin for Google Kubernetes Engine (GKE)."
      ],
      "metadata": {
        "id": "MJkq3GeyosAI"
      },
      "id": "MJkq3GeyosAI"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n",
        "sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n",
        "apt-get update && apt-get install apt-transport-https ca-certificates gnupg\n",
        "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg\n",
        "echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
        "apt-get update && sudo apt-get install google-cloud-cli-gke-gcloud-auth-plugin\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tiGscdCpo2KC"
      },
      "id": "tiGscdCpo2KC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Replace** <CLUSTER_NAME> with your cluster name, e.g. sqlgen-cluster. Retrieve the GKE cluster's credentials using the **gcloud** command:"
      ],
      "metadata": {
        "id": "Q8x_lxTXrZTi"
      },
      "id": "Q8x_lxTXrZTi"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "export KUBERNETES_CLUSTER_NAME=<CLUSTER_NAME>\n",
        "gcloud container clusters get-credentials $KUBERNETES_CLUSTER_NAME --region $GOOGLE_CLOUD_REGION"
      ],
      "metadata": {
        "id": "n1c8Fsk1rcrK"
      },
      "id": "n1c8Fsk1rcrK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an .env file with environment variables required for connecting to Postgresql and LLM runtime in a Kubernetes cluster."
      ],
      "metadata": {
        "id": "USwAo2Nwr9wQ"
      },
      "id": "USwAo2Nwr9wQ"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo POSTGRES_ENDPOINT=$(kubectl get svc postgres-ilb -n postgres --output jsonpath=\"{.status.loadBalancer.ingress[0].ip}\") > .env\n",
        "echo LLM_ENDPOINT=http://$(kubectl get svc llm-ilb -n llm --output jsonpath=\"{.status.loadBalancer.ingress[0].ip}\"):8000 >> .env\n",
        "echo DATABASE_NAME=mydatabase >> .env\n",
        "echo DBUSERNAME=$(kubectl get secret mydatabaseowner.my-cluster.credentials.postgresql.acid.zalan.do -n postgres --template={{.data.username}} | base64 -d) >> .env\n",
        "echo DBPASSWORD=$(kubectl get secret mydatabaseowner.my-cluster.credentials.postgresql.acid.zalan.do -n postgres --template={{.data.password}} | base64 -d) >> .env"
      ],
      "metadata": {
        "id": "6lo6M4AqsAru"
      },
      "id": "6lo6M4AqsAru",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required python libraries:"
      ],
      "metadata": {
        "id": "xX-aQbhl4g4C"
      },
      "id": "xX-aQbhl4g4C"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install python-dotenv psycopg-binary psycopg tabulate text-generation langchain langchain-community"
      ],
      "metadata": {
        "id": "uwspErB44izG"
      },
      "id": "uwspErB44izG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import python libraries:"
      ],
      "metadata": {
        "id": "9acUeBu15quS"
      },
      "id": "9acUeBu15quS"
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import psycopg\n",
        "import os\n",
        "from tabulate import tabulate\n",
        "from langchain_community.llms import HuggingFaceTextGenInference\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "oqq4NDgvt-XT"
      },
      "id": "oqq4NDgvt-XT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load environment variables from the .env file, establish a connection to a PostgreSQL database and retrieve information about the schema of the public tables."
      ],
      "metadata": {
        "id": "Ef9FSgl55_MI"
      },
      "id": "Ef9FSgl55_MI"
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "\n",
        "conn = psycopg.connect(\n",
        "    dbname=os.environ.get(\"DATABASE_NAME\"),\n",
        "    host=os.environ.get(\"POSTGRES_ENDPOINT\"),\n",
        "    user=os.environ.get(\"DBUSERNAME\"),\n",
        "    password=os.environ.get(\"DBPASSWORD\"),\n",
        "    autocommit=True)\n",
        "\n",
        "db_schema = conn.execute(\"SELECT table_name, column_name as Columns, data_type as DataTypes FROM  information_schema.columns where table_name NOT LIKE 'pg_stat%' AND table_schema='public' order by table_name,column_name;\")\n",
        "colnames = [desc[0] for desc in db_schema.description]\n",
        "db_schema_formatted=tabulate(db_schema.fetchall(), headers=colnames, tablefmt='psql')\n"
      ],
      "metadata": {
        "id": "7M-IQwbM6AgY"
      },
      "id": "7M-IQwbM6AgY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializes the Hugging Face TGI connection for text generation.\n",
        "\n",
        "Set up two prompts: the first one is for generating SQL commands based on user queries, while the second prompt is for generating responses based on user queries and PostgreSQL replies."
      ],
      "metadata": {
        "id": "9bLuIcNU6sz7"
      },
      "id": "9bLuIcNU6sz7"
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceTextGenInference(\n",
        "    inference_server_url=os.environ.get(\"LLM_ENDPOINT\"),\n",
        "    temperature=0.5,\n",
        "    top_k=5,\n",
        "    top_p=0.5,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "\n",
        "sql_prompt_template = PromptTemplate.from_template(\"\"\"\n",
        "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "    You are a helpful AI assistant that can transform user queries into SQL commands to retrieve the data from the Postgresql database. The database has the next tables schema:\n",
        "    {db_schema}\n",
        "    Please prepare and return only the SQL command, based on the user query, without any formatting or newlines. The answer must contain only valid SQL command.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    {query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\")\n",
        "\n",
        "final_prompt_template = PromptTemplate.from_template(\"\"\"\n",
        "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "    You are a helpful AI assistant that can understand Postgresql replies and explain this data to the user. The database has the next tables schema:\n",
        "    {db_schema}\n",
        "    User query: {query}\n",
        "    Postgresql reply:\n",
        "    {postgres_reply}\n",
        "    Base your answer on the provided user query and Postgresql reply.\n",
        "    Generate a draft response using the selected information.\n",
        "    It should be easy to understand your answer. Don't add any introductory words, start answering right away.\n",
        "    Keep your answer to a one or two sentences (if possible) that specifically answers the user's question. If not - try to keep the answer short, summarizing the returned data.\n",
        "    Generate your final response after adjusting it to increase accuracy and relevance.\n",
        "    Now only show your final response!\n",
        "    If you do not know the answer or context is not relevant, response with \"I don't know\".\n",
        "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "RKLr5riV6uZW"
      },
      "id": "RKLr5riV6uZW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure two functions to interacte with the PostgreSQL database and the TGI runtime."
      ],
      "metadata": {
        "id": "vdtggWOO7Wef"
      },
      "id": "vdtggWOO7Wef"
    },
    {
      "cell_type": "code",
      "source": [
        "def postgres_query(query):\n",
        "    try:\n",
        "        postgres_reply = conn.execute(query)\n",
        "    except psycopg.Error as e:\n",
        "        print(\"Unable to process query\")\n",
        "        return False\n",
        "    colnames = [desc[0] for desc in postgres_reply.description]\n",
        "    postgres_reply_data = postgres_reply.fetchall()\n",
        "    if postgres_reply_data == []:\n",
        "        print(\"Received empty SQL reply\")\n",
        "        return False\n",
        "    postgres_reply_formatted=tabulate(postgres_reply_data, headers=colnames, tablefmt='psql')\n",
        "    return postgres_reply_formatted\n",
        "\n",
        "\n",
        "def llm_query(query):\n",
        "    sql_prompt_value=sql_prompt_template.format(db_schema=db_schema_formatted, query=query)\n",
        "    sql_query=llm.invoke(sql_prompt_value)\n",
        "    postgres_reply=postgres_query(sql_query)\n",
        "    if postgres_reply == False:\n",
        "        return \"Try another query\"\n",
        "    final_prompt_value=final_prompt_template.format(db_schema=db_schema_formatted, query=query, postgres_reply=postgres_reply)\n",
        "    return llm.invoke(final_prompt_value)"
      ],
      "metadata": {
        "id": "a_6XVEtH7dFv"
      },
      "id": "a_6XVEtH7dFv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run some queries to demonstrate the model's ability to generate SQL commands from user queries and provide responses based on the PostgreSQL replies:"
      ],
      "metadata": {
        "id": "LMMBr-gu7g0I"
      },
      "id": "LMMBr-gu7g0I"
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_query(\"Please calculate the total sum of all John transactions.\"))\n",
        "print(llm_query(\"Which woman spent more money in 2023 and how much?\"))\n",
        "print(llm_query(\"What is the capital of Great Britain?\"))\n",
        "print(llm_query(\"Who spent more money on electronics in last month?\"))\n",
        "print(llm_query(\"Who spent more money on electronics in last year?\"))\n",
        "print(llm_query(\"Give me top 3 buyers of clothing. How much money each person spent?\"))"
      ],
      "metadata": {
        "id": "HQZ1gUYY7iLn"
      },
      "id": "HQZ1gUYY7iLn",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "notebook.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}